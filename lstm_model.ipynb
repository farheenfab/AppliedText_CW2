{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Archisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Archisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text_without_punctuation)\n",
    "    return normalized_text\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text, method='snowballstemming'):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Define the set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [remove_punctuation(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    if method == 'snowballstemming':\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'porterstemming':\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'lemmatization':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ssprocessing_text'] = train_df['Review'].apply(preprocess_text, method='snowballstemming')\n",
    "train_df['psprocessing_text'] = train_df['Review'].apply(preprocess_text, method='porterstemming')\n",
    "train_df['lemprocessing_text'] = train_df['Review'].apply(preprocess_text, method='lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "      <th>psprocessing_text</th>\n",
       "      <th>lemprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these glitter pens. They sparkle deligh...</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "      <td>love glitter pen sparkle delightfully page bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>It works well with my machine.  I use mostly c...</td>\n",
       "      <td>work well machin use most cone</td>\n",
       "      <td>work well machin use mostli cone</td>\n",
       "      <td>work well machine use mostly cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a great assortment of colors, though t...</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "      <td>great assortment color though lot pink mix sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just what I was looking for.</td>\n",
       "      <td>look</td>\n",
       "      <td>look</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I make 400 birds for the hospital each month.</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "      <td>make 400 bird hospital month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>Lovely yarn, very fast delivery, I am so pleas...</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "      <td>lovely yarn fast delivery pleased buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>These perfectly match some charms I ordered to...</td>\n",
       "      <td>perfect match charm order make photo pendant like</td>\n",
       "      <td>perfectli match charm order make photo pendant...</td>\n",
       "      <td>perfectly match charm ordered make photo penda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for my project.</td>\n",
       "      <td>perfect project</td>\n",
       "      <td>perfect project</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>great product</td>\n",
       "      <td>great product</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>A little pricy for a pen case but it's definit...</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "      <td>little pricy pen case definitely sturdy fabric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                             Review  \\\n",
       "0           5  I love these glitter pens. They sparkle deligh...   \n",
       "1           5  It works well with my machine.  I use mostly c...   \n",
       "2           5  This is a great assortment of colors, though t...   \n",
       "3           5                       Just what I was looking for.   \n",
       "4           5      I make 400 birds for the hospital each month.   \n",
       "...       ...                                                ...   \n",
       "9995        5  Lovely yarn, very fast delivery, I am so pleas...   \n",
       "9996        5  These perfectly match some charms I ordered to...   \n",
       "9997        5                            Perfect for my project.   \n",
       "9998        5                                     Great product!   \n",
       "9999        5  A little pricy for a pen case but it's definit...   \n",
       "\n",
       "                                      ssprocessing_text  \\\n",
       "0     love glitter pen sparkl delight page brilliant...   \n",
       "1                        work well machin use most cone   \n",
       "2     great assort color though lot pink mix still c...   \n",
       "3                                                  look   \n",
       "4                            make 400 bird hospit month   \n",
       "...                                                 ...   \n",
       "9995                  love yarn fast deliveri pleas buy   \n",
       "9996  perfect match charm order make photo pendant like   \n",
       "9997                                    perfect project   \n",
       "9998                                      great product   \n",
       "9999  littl prici pen case definit sturdi fabric out...   \n",
       "\n",
       "                                      psprocessing_text  \\\n",
       "0     love glitter pen sparkl delight page brilliant...   \n",
       "1                      work well machin use mostli cone   \n",
       "2     great assort color though lot pink mix still c...   \n",
       "3                                                  look   \n",
       "4                            make 400 bird hospit month   \n",
       "...                                                 ...   \n",
       "9995                  love yarn fast deliveri pleas buy   \n",
       "9996  perfectli match charm order make photo pendant...   \n",
       "9997                                    perfect project   \n",
       "9998                                      great product   \n",
       "9999  littl prici pen case definit sturdi fabric out...   \n",
       "\n",
       "                                     lemprocessing_text  \n",
       "0     love glitter pen sparkle delightfully page bri...  \n",
       "1                     work well machine use mostly cone  \n",
       "2     great assortment color though lot pink mix sti...  \n",
       "3                                               looking  \n",
       "4                          make 400 bird hospital month  \n",
       "...                                                 ...  \n",
       "9995              lovely yarn fast delivery pleased buy  \n",
       "9996  perfectly match charm ordered make photo penda...  \n",
       "9997                                    perfect project  \n",
       "9998                                      great product  \n",
       "9999  little pricy pen case definitely sturdy fabric...  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_df = train_df.copy()\n",
    "\n",
    "# # Iterate over each column\n",
    "# for column in train_df.columns:\n",
    "#     # Check if the column is not 'overall'\n",
    "#     if column != 'overall':\n",
    "#         # Tokenize values in the column\n",
    "#         tokenized_values = train_df[column].apply(lambda x: word_tokenize(x))\n",
    "#         # Add tokenized values to the new DataFrame\n",
    "#         tokenized_df[column] = tokenized_values\n",
    "\n",
    "# tokenized_df\n",
    "\n",
    "# word2vec_model = gensim.models.Word2Vec(subset_df[\"tokens\"].tolist(), min_count=5, window=9, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use most cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect match charm order make photo pendant like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                  ssprocessing_text\n",
       "0           5  love glitter pen sparkl delight page brilliant...\n",
       "1           5                     work well machin use most cone\n",
       "2           5  great assort color though lot pink mix still c...\n",
       "3           5                                               look\n",
       "4           5                         make 400 bird hospit month\n",
       "...       ...                                                ...\n",
       "9995        5                  love yarn fast deliveri pleas buy\n",
       "9996        5  perfect match charm order make photo pendant like\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  littl prici pen case definit sturdi fabric out...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ss = train_df[['overall', 'ssprocessing_text']].copy()\n",
    "train_df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>psprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use mostli cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfectli match charm order make photo pendant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                  psprocessing_text\n",
       "0           5  love glitter pen sparkl delight page brilliant...\n",
       "1           5                   work well machin use mostli cone\n",
       "2           5  great assort color though lot pink mix still c...\n",
       "3           5                                               look\n",
       "4           5                         make 400 bird hospit month\n",
       "...       ...                                                ...\n",
       "9995        5                  love yarn fast deliveri pleas buy\n",
       "9996        5  perfectli match charm order make photo pendant...\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  littl prici pen case definit sturdi fabric out...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ps = train_df[['overall', 'psprocessing_text']].copy()\n",
    "train_df_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>lemprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkle delightfully page bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machine use mostly cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assortment color though lot pink mix sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospital month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>lovely yarn fast delivery pleased buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfectly match charm ordered make photo penda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>little pricy pen case definitely sturdy fabric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                 lemprocessing_text\n",
       "0           5  love glitter pen sparkle delightfully page bri...\n",
       "1           5                  work well machine use mostly cone\n",
       "2           5  great assortment color though lot pink mix sti...\n",
       "3           5                                            looking\n",
       "4           5                       make 400 bird hospital month\n",
       "...       ...                                                ...\n",
       "9995        5              lovely yarn fast delivery pleased buy\n",
       "9996        5  perfectly match charm ordered make photo penda...\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  little pricy pen case definitely sturdy fabric...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_lem = train_df[['overall', 'lemprocessing_text']].copy()\n",
    "train_df_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Withoud word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 41s 144ms/step - loss: 0.7815 - accuracy: 0.7651 - val_loss: 0.6753 - val_accuracy: 0.7795\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.5948 - accuracy: 0.7864 - val_loss: 0.6828 - val_accuracy: 0.7750\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.4855 - accuracy: 0.8257 - val_loss: 0.7295 - val_accuracy: 0.7640\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 0.3941 - accuracy: 0.8602 - val_loss: 0.7956 - val_accuracy: 0.7555\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.3126 - accuracy: 0.8942 - val_loss: 0.8855 - val_accuracy: 0.7525\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 0.2590 - accuracy: 0.9121 - val_loss: 1.0090 - val_accuracy: 0.7180\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 32s 130ms/step - loss: 0.2141 - accuracy: 0.9299 - val_loss: 1.0203 - val_accuracy: 0.7370\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.1777 - accuracy: 0.9436 - val_loss: 1.1877 - val_accuracy: 0.7200\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.1530 - accuracy: 0.9520 - val_loss: 1.2791 - val_accuracy: 0.7195\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.1392 - accuracy: 0.9546 - val_loss: 1.3383 - val_accuracy: 0.7180\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.1266 - accuracy: 0.9600 - val_loss: 1.4758 - val_accuracy: 0.7285\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.1213 - accuracy: 0.9621 - val_loss: 1.4302 - val_accuracy: 0.6930\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 0.1151 - accuracy: 0.9619 - val_loss: 1.4476 - val_accuracy: 0.7125\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.1101 - accuracy: 0.9647 - val_loss: 1.5489 - val_accuracy: 0.7470\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.1192 - accuracy: 0.9606 - val_loss: 1.4634 - val_accuracy: 0.7160\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.1043 - accuracy: 0.9659 - val_loss: 1.5658 - val_accuracy: 0.7360\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.0987 - accuracy: 0.9676 - val_loss: 1.6855 - val_accuracy: 0.7220\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.0930 - accuracy: 0.9690 - val_loss: 1.7065 - val_accuracy: 0.7145\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.0909 - accuracy: 0.9703 - val_loss: 1.6443 - val_accuracy: 0.7245\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.1025 - accuracy: 0.9651 - val_loss: 1.6059 - val_accuracy: 0.7120\n",
      "63/63 [==============================] - 3s 49ms/step - loss: 1.6059 - accuracy: 0.7120\n",
      "Test Accuracy: 0.7120000123977661\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model = LSTM_model(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 25s 91ms/step - loss: 0.8594 - accuracy: 0.7638 - val_loss: 0.7915 - val_accuracy: 0.7800\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.8274 - accuracy: 0.7664 - val_loss: 0.7895 - val_accuracy: 0.7800\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.8247 - accuracy: 0.7664 - val_loss: 0.7901 - val_accuracy: 0.7800\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.8235 - accuracy: 0.7664 - val_loss: 0.7937 - val_accuracy: 0.7800\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.8235 - accuracy: 0.7661 - val_loss: 0.7945 - val_accuracy: 0.7800\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.8234 - accuracy: 0.7664 - val_loss: 0.7922 - val_accuracy: 0.7800\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 0.8246 - accuracy: 0.7664 - val_loss: 0.7898 - val_accuracy: 0.7800\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.8235 - accuracy: 0.7664 - val_loss: 0.7962 - val_accuracy: 0.7800\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.8241 - accuracy: 0.7664 - val_loss: 0.7899 - val_accuracy: 0.7800\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.8217 - accuracy: 0.7664 - val_loss: 0.7914 - val_accuracy: 0.7800\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.8207 - accuracy: 0.7664 - val_loss: 0.7948 - val_accuracy: 0.7800\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.8215 - accuracy: 0.7664 - val_loss: 0.7950 - val_accuracy: 0.7800\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.8219 - accuracy: 0.7663 - val_loss: 0.7890 - val_accuracy: 0.7800\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.8201 - accuracy: 0.7664 - val_loss: 0.7882 - val_accuracy: 0.7800\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.8192 - accuracy: 0.7664 - val_loss: 0.7857 - val_accuracy: 0.7800\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.8175 - accuracy: 0.7664 - val_loss: 0.7879 - val_accuracy: 0.7800\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.8172 - accuracy: 0.7664 - val_loss: 0.7870 - val_accuracy: 0.7800\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.8185 - accuracy: 0.7664 - val_loss: 0.7905 - val_accuracy: 0.7800\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.8169 - accuracy: 0.7664 - val_loss: 0.7860 - val_accuracy: 0.7800\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.8170 - accuracy: 0.7664 - val_loss: 0.7860 - val_accuracy: 0.7800\n",
      "63/63 [==============================] - 3s 52ms/step - loss: 0.7860 - accuracy: 0.7800\n",
      "Test Accuracy with Word2Vec embeddings: 0.7799999713897705\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec = LSTM_model_with_Word2Vec(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model_lem.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model_lem.model\\assets\n"
     ]
    }
   ],
   "source": [
    "lstm_model_with_word2vec.save('lstm_model_lem.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 28s 99ms/step - loss: 0.8660 - accuracy: 0.7635 - val_loss: 0.7909 - val_accuracy: 0.7800\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.8269 - accuracy: 0.7664 - val_loss: 0.7939 - val_accuracy: 0.7800\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.8228 - accuracy: 0.7664 - val_loss: 0.7974 - val_accuracy: 0.7800\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.8221 - accuracy: 0.7664 - val_loss: 0.7918 - val_accuracy: 0.7800\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 27s 110ms/step - loss: 0.8236 - accuracy: 0.7664 - val_loss: 0.7880 - val_accuracy: 0.7800\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 29s 114ms/step - loss: 0.8243 - accuracy: 0.7660 - val_loss: 0.7905 - val_accuracy: 0.7800\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.8203 - accuracy: 0.7664 - val_loss: 0.7968 - val_accuracy: 0.7800\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.8216 - accuracy: 0.7664 - val_loss: 0.7890 - val_accuracy: 0.7800\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.8233 - accuracy: 0.7664 - val_loss: 0.7884 - val_accuracy: 0.7800\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 0.8222 - accuracy: 0.7664 - val_loss: 0.7931 - val_accuracy: 0.7800\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 0.8210 - accuracy: 0.7664 - val_loss: 0.7907 - val_accuracy: 0.7800\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 30s 122ms/step - loss: 0.8214 - accuracy: 0.7664 - val_loss: 0.7858 - val_accuracy: 0.7800\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 0.8176 - accuracy: 0.7665 - val_loss: 0.7948 - val_accuracy: 0.7800\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.8207 - accuracy: 0.7663 - val_loss: 0.7893 - val_accuracy: 0.7805\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.8173 - accuracy: 0.7665 - val_loss: 0.7870 - val_accuracy: 0.7800\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.8218 - accuracy: 0.7664 - val_loss: 0.7871 - val_accuracy: 0.7800\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.8187 - accuracy: 0.7664 - val_loss: 0.7883 - val_accuracy: 0.7800\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.8178 - accuracy: 0.7665 - val_loss: 0.7876 - val_accuracy: 0.7800\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.8165 - accuracy: 0.7664 - val_loss: 0.7865 - val_accuracy: 0.7805\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 0.8170 - accuracy: 0.7666 - val_loss: 0.7880 - val_accuracy: 0.7800\n",
      "63/63 [==============================] - 4s 61ms/step - loss: 0.7880 - accuracy: 0.7800\n",
      "Test Accuracy with Word2Vec embeddings: 0.7799999713897705\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['psprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec = LSTM_model_with_Word2Vec(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
