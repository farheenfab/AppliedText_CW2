{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_df = train_df.copy()\n",
    "\n",
    "# # Iterate over each column\n",
    "# for column in train_df.columns:\n",
    "#     # Check if the column is not 'overall'\n",
    "#     if column != 'overall':\n",
    "#         # Tokenize values in the column\n",
    "#         tokenized_values = train_df[column].apply(lambda x: word_tokenize(x))\n",
    "#         # Add tokenized values to the new DataFrame\n",
    "#         tokenized_df[column] = tokenized_values\n",
    "\n",
    "# tokenized_df\n",
    "\n",
    "# word2vec_model = gensim.models.Word2Vec(subset_df[\"tokens\"].tolist(), min_count=5, window=9, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use most cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370850</th>\n",
       "      <td>5</td>\n",
       "      <td>love die make great background card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370851</th>\n",
       "      <td>5</td>\n",
       "      <td>love daric emboss folder darci folder reason e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370852</th>\n",
       "      <td>5</td>\n",
       "      <td>order add earthi marker previous order want br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370853</th>\n",
       "      <td>4</td>\n",
       "      <td>made perfect white color blend tini amount yel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370854</th>\n",
       "      <td>5</td>\n",
       "      <td>use make photo canvas christma product realli ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369731 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                  ssprocessing_text\n",
       "0             5  love glitter pen sparkl delight page brilliant...\n",
       "1             5                     work well machin use most cone\n",
       "2             5  great assort color though lot pink mix still c...\n",
       "3             5                                               look\n",
       "4             5                         make 400 bird hospit month\n",
       "...         ...                                                ...\n",
       "370850        5                love die make great background card\n",
       "370851        5  love daric emboss folder darci folder reason e...\n",
       "370852        5  order add earthi marker previous order want br...\n",
       "370853        4  made perfect white color blend tini amount yel...\n",
       "370854        5  use make photo canvas christma product realli ...\n",
       "\n",
       "[369731 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ss = train_df[['overall', 'ssprocessing_text']].copy()\n",
    "train_df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>psprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use mostli cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370850</th>\n",
       "      <td>5</td>\n",
       "      <td>love die make great background card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370851</th>\n",
       "      <td>5</td>\n",
       "      <td>love daric emboss folder darci folder reason e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370852</th>\n",
       "      <td>5</td>\n",
       "      <td>order add earthi marker previous order want br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370853</th>\n",
       "      <td>4</td>\n",
       "      <td>made perfect white color blend tini amount yel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370854</th>\n",
       "      <td>5</td>\n",
       "      <td>use make photo canvas christma product realli ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369731 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                  psprocessing_text\n",
       "0             5  love glitter pen sparkl delight page brilliant...\n",
       "1             5                   work well machin use mostli cone\n",
       "2             5  great assort color though lot pink mix still c...\n",
       "3             5                                               look\n",
       "4             5                         make 400 bird hospit month\n",
       "...         ...                                                ...\n",
       "370850        5                love die make great background card\n",
       "370851        5  love daric emboss folder darci folder reason e...\n",
       "370852        5  order add earthi marker previous order want br...\n",
       "370853        4  made perfect white color blend tini amount yel...\n",
       "370854        5  use make photo canvas christma product realli ...\n",
       "\n",
       "[369731 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ps = train_df[['overall', 'psprocessing_text']].copy()\n",
    "train_df_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>lemprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkle delightfully page bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machine use mostly cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assortment color though lot pink mix sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospital month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370850</th>\n",
       "      <td>5</td>\n",
       "      <td>love dy make great background card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370851</th>\n",
       "      <td>5</td>\n",
       "      <td>love darice embossing folder darcie folder rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370852</th>\n",
       "      <td>5</td>\n",
       "      <td>ordered add earthy marker previously ordered w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370853</th>\n",
       "      <td>4</td>\n",
       "      <td>made perfect white color blending tiny amount ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370854</th>\n",
       "      <td>5</td>\n",
       "      <td>used make photo canvas christmas product reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369731 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                 lemprocessing_text\n",
       "0             5  love glitter pen sparkle delightfully page bri...\n",
       "1             5                  work well machine use mostly cone\n",
       "2             5  great assortment color though lot pink mix sti...\n",
       "3             5                                            looking\n",
       "4             5                       make 400 bird hospital month\n",
       "...         ...                                                ...\n",
       "370850        5                 love dy make great background card\n",
       "370851        5  love darice embossing folder darcie folder rea...\n",
       "370852        5  ordered add earthy marker previously ordered w...\n",
       "370853        4  made perfect white color blending tiny amount ...\n",
       "370854        5  used make photo canvas christmas product reall...\n",
       "\n",
       "[369731 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_lem = train_df[['overall', 'lemprocessing_text']].copy()\n",
    "train_df_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "def early_stopping(monitor='val_loss', min_delta=0, patience=5, mode='auto'):\n",
    "    return EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, mode=mode)\n",
    "\n",
    "# Function to create step decay learning rate scheduler\n",
    "def step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5):\n",
    "    def scheduler(epoch):\n",
    "        return initial_lr * np.power(drop_factor, np.floor((1 + epoch) / epochs_drop))\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4622/4622 [==============================] - 1119s 241ms/step - loss: 0.6100 - accuracy: 0.7881 - val_loss: 0.5668 - val_accuracy: 0.7990 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "4622/4622 [==============================] - 1148s 248ms/step - loss: 0.5404 - accuracy: 0.8072 - val_loss: 0.5633 - val_accuracy: 0.7986 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "4622/4622 [==============================] - 1151s 249ms/step - loss: 0.4998 - accuracy: 0.8227 - val_loss: 0.5666 - val_accuracy: 0.8043 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "4622/4622 [==============================] - 1153s 249ms/step - loss: 0.4582 - accuracy: 0.8379 - val_loss: 0.5706 - val_accuracy: 0.8033 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "4622/4622 [==============================] - 1238s 268ms/step - loss: 0.4003 - accuracy: 0.8597 - val_loss: 0.6219 - val_accuracy: 0.7998 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "4622/4622 [==============================] - 1159s 251ms/step - loss: 0.3719 - accuracy: 0.8704 - val_loss: 0.6506 - val_accuracy: 0.7981 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "4622/4622 [==============================] - 1156s 250ms/step - loss: 0.3498 - accuracy: 0.8784 - val_loss: 0.6900 - val_accuracy: 0.7961 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 80s 35ms/step - loss: 0.6900 - accuracy: 0.7961\n",
      "Test Accuracy: 0.7961107492446899\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model = LSTM_model(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 154s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_lem1.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_tm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4622/4622 [==============================] - 1156s 249ms/step - loss: 0.6125 - accuracy: 0.7868 - val_loss: 0.5730 - val_accuracy: 0.7950 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "4622/4622 [==============================] - 1058s 229ms/step - loss: 0.5505 - accuracy: 0.8039 - val_loss: 0.5668 - val_accuracy: 0.7982 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "4622/4622 [==============================] - 1063s 230ms/step - loss: 0.5153 - accuracy: 0.8164 - val_loss: 0.5658 - val_accuracy: 0.8018 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "4622/4622 [==============================] - 1065s 230ms/step - loss: 0.4829 - accuracy: 0.8294 - val_loss: 0.5688 - val_accuracy: 0.8021 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "4622/4622 [==============================] - 1063s 230ms/step - loss: 0.4342 - accuracy: 0.8469 - val_loss: 0.5957 - val_accuracy: 0.8035 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "4622/4622 [==============================] - 1095s 237ms/step - loss: 0.4103 - accuracy: 0.8560 - val_loss: 0.6262 - val_accuracy: 0.7983 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "4622/4622 [==============================] - 1093s 237ms/step - loss: 0.3899 - accuracy: 0.8633 - val_loss: 0.6430 - val_accuracy: 0.7995 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "4622/4622 [==============================] - 1087s 235ms/step - loss: 0.3721 - accuracy: 0.8700 - val_loss: 0.6764 - val_accuracy: 0.7979 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 76s 33ms/step - loss: 0.6764 - accuracy: 0.7979\n",
      "Test Accuracy: 0.7979093194007874\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['psprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model_ps(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model_ps = LSTM_model_ps(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model_ps.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_ps.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 135s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model_ps.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_ps.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_ps_tm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4622/4622 [==============================] - 1151s 248ms/step - loss: 0.6139 - accuracy: 0.7862 - val_loss: 0.5764 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "4622/4622 [==============================] - 1198s 259ms/step - loss: 0.5524 - accuracy: 0.8037 - val_loss: 0.5602 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "4622/4622 [==============================] - 1210s 262ms/step - loss: 0.5176 - accuracy: 0.8159 - val_loss: 0.5613 - val_accuracy: 0.8022 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "4622/4622 [==============================] - 1282s 277ms/step - loss: 0.4850 - accuracy: 0.8280 - val_loss: 0.5707 - val_accuracy: 0.8028 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "4622/4622 [==============================] - 1232s 267ms/step - loss: 0.4375 - accuracy: 0.8460 - val_loss: 0.5990 - val_accuracy: 0.8012 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "4622/4622 [==============================] - 1297s 281ms/step - loss: 0.4133 - accuracy: 0.8554 - val_loss: 0.6177 - val_accuracy: 0.7989 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "4622/4622 [==============================] - 1180s 255ms/step - loss: 0.3933 - accuracy: 0.8629 - val_loss: 0.6448 - val_accuracy: 0.7992 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 87s 37ms/step - loss: 0.6448 - accuracy: 0.7992\n",
      "Test Accuracy: 0.7991940379142761\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['ssprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model_ss(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model_ss = LSTM_model_ss(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model_ss.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 152s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model_ss.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_ss.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_ss_tm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define LSTM model with Word2Vec embeddings\n",
    "# def LSTM_model2_with_Word2Vec(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "#     model.add(LSTM(200))\n",
    "#     model.add(Dropout(0.35))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create and compile the LSTM model with Word2Vec embeddings\n",
    "# lstm_model2_with_word2vec = LSTM_model2_with_Word2Vec(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# # Train the model\n",
    "# lstm_model2_with_word2vec.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = lstm_model2_with_word2vec.evaluate(X_test, y_test)\n",
    "# print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the snowball stemmer ave us the highest accuracy with the LSTM and without word embeddings, we use the word2vec model along with LSTM only on the snowball stemmer preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec with Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4622/4622 [==============================] - 1438s 310ms/step - loss: 0.6153 - accuracy: 0.7870 - val_loss: 0.5743 - val_accuracy: 0.7962 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "4622/4622 [==============================] - 1303s 282ms/step - loss: 0.5530 - accuracy: 0.8035 - val_loss: 0.5636 - val_accuracy: 0.8001 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "4622/4622 [==============================] - 1272s 275ms/step - loss: 0.5190 - accuracy: 0.8153 - val_loss: 0.5636 - val_accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "4622/4622 [==============================] - 1303s 282ms/step - loss: 0.4861 - accuracy: 0.8276 - val_loss: 0.5698 - val_accuracy: 0.8033 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "4622/4622 [==============================] - 1342s 290ms/step - loss: 0.4365 - accuracy: 0.8461 - val_loss: 0.5981 - val_accuracy: 0.8019 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 134s 58ms/step - loss: 0.5981 - accuracy: 0.8019\n",
      "Test Accuracy with Word2Vec embeddings: 0.8019121885299683\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['ssprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec_ss(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec_ss = LSTM_model_with_Word2Vec_ss(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec_ss.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 199s 51ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model_with_word2vec_ss.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_w2v_ss.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_w2v_ss_tm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LSTM_model2_with_Word2Vec_ss(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "#     model.add(LSTM(200))\n",
    "#     model.add(Dropout(0.35))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create and compile the LSTM model with Word2Vec embeddings\n",
    "# lstm_model2_with_word2vec_ss = LSTM_model2_with_Word2Vec_ss(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# # Train the model\n",
    "# lstm_model2_with_word2vec_ss.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = lstm_model2_with_word2vec_ss.evaluate(X_test, y_test)\n",
    "# print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4622/4622 [==============================] - 1866s 403ms/step - loss: 0.6110 - accuracy: 0.7873 - val_loss: 0.5700 - val_accuracy: 0.7972 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "4622/4622 [==============================] - 1850s 400ms/step - loss: 0.5424 - accuracy: 0.8070 - val_loss: 0.5606 - val_accuracy: 0.7997 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "4622/4622 [==============================] - 1799s 389ms/step - loss: 0.5021 - accuracy: 0.8215 - val_loss: 0.5648 - val_accuracy: 0.8020 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "4622/4622 [==============================] - 5980s 1s/step - loss: 0.4609 - accuracy: 0.8369 - val_loss: 0.5782 - val_accuracy: 0.8012 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "4622/4622 [==============================] - 1643s 355ms/step - loss: 0.4036 - accuracy: 0.8577 - val_loss: 0.6103 - val_accuracy: 0.8013 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 193s 84ms/step - loss: 0.6103 - accuracy: 0.8013\n",
      "Test Accuracy with Word2Vec embeddings: 0.8013306856155396\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec_lem(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec_lem = LSTM_model_with_Word2Vec_lem(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec_lem.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec_lem.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 201s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model_with_word2vec_lem.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_w2v_lem.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_w2v_lem_tm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
