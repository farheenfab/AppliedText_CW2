{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Archisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Archisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text_without_punctuation)\n",
    "    return normalized_text\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text, method='snowballstemming'):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Define the set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [remove_punctuation(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    if method == 'snowballstemming':\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'porterstemming':\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'lemmatization':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ssprocessing_text'] = train_df['Review'].apply(preprocess_text, method='snowballstemming')\n",
    "train_df['psprocessing_text'] = train_df['Review'].apply(preprocess_text, method='porterstemming')\n",
    "train_df['lemprocessing_text'] = train_df['Review'].apply(preprocess_text, method='lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "      <th>psprocessing_text</th>\n",
       "      <th>lemprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these glitter pens. They sparkle deligh...</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "      <td>love glitter pen sparkle delightfully page bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>It works well with my machine.  I use mostly c...</td>\n",
       "      <td>work well machin use most cone</td>\n",
       "      <td>work well machin use mostli cone</td>\n",
       "      <td>work well machine use mostly cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a great assortment of colors, though t...</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "      <td>great assortment color though lot pink mix sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just what I was looking for.</td>\n",
       "      <td>look</td>\n",
       "      <td>look</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I make 400 birds for the hospital each month.</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "      <td>make 400 bird hospital month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>Lovely yarn, very fast delivery, I am so pleas...</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "      <td>lovely yarn fast delivery pleased buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>These perfectly match some charms I ordered to...</td>\n",
       "      <td>perfect match charm order make photo pendant like</td>\n",
       "      <td>perfectli match charm order make photo pendant...</td>\n",
       "      <td>perfectly match charm ordered make photo penda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for my project.</td>\n",
       "      <td>perfect project</td>\n",
       "      <td>perfect project</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>great product</td>\n",
       "      <td>great product</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>A little pricy for a pen case but it's definit...</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "      <td>little pricy pen case definitely sturdy fabric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                             Review  \\\n",
       "0           5  I love these glitter pens. They sparkle deligh...   \n",
       "1           5  It works well with my machine.  I use mostly c...   \n",
       "2           5  This is a great assortment of colors, though t...   \n",
       "3           5                       Just what I was looking for.   \n",
       "4           5      I make 400 birds for the hospital each month.   \n",
       "...       ...                                                ...   \n",
       "9995        5  Lovely yarn, very fast delivery, I am so pleas...   \n",
       "9996        5  These perfectly match some charms I ordered to...   \n",
       "9997        5                            Perfect for my project.   \n",
       "9998        5                                     Great product!   \n",
       "9999        5  A little pricy for a pen case but it's definit...   \n",
       "\n",
       "                                      ssprocessing_text  \\\n",
       "0     love glitter pen sparkl delight page brilliant...   \n",
       "1                        work well machin use most cone   \n",
       "2     great assort color though lot pink mix still c...   \n",
       "3                                                  look   \n",
       "4                            make 400 bird hospit month   \n",
       "...                                                 ...   \n",
       "9995                  love yarn fast deliveri pleas buy   \n",
       "9996  perfect match charm order make photo pendant like   \n",
       "9997                                    perfect project   \n",
       "9998                                      great product   \n",
       "9999  littl prici pen case definit sturdi fabric out...   \n",
       "\n",
       "                                      psprocessing_text  \\\n",
       "0     love glitter pen sparkl delight page brilliant...   \n",
       "1                      work well machin use mostli cone   \n",
       "2     great assort color though lot pink mix still c...   \n",
       "3                                                  look   \n",
       "4                            make 400 bird hospit month   \n",
       "...                                                 ...   \n",
       "9995                  love yarn fast deliveri pleas buy   \n",
       "9996  perfectli match charm order make photo pendant...   \n",
       "9997                                    perfect project   \n",
       "9998                                      great product   \n",
       "9999  littl prici pen case definit sturdi fabric out...   \n",
       "\n",
       "                                     lemprocessing_text  \n",
       "0     love glitter pen sparkle delightfully page bri...  \n",
       "1                     work well machine use mostly cone  \n",
       "2     great assortment color though lot pink mix sti...  \n",
       "3                                               looking  \n",
       "4                          make 400 bird hospital month  \n",
       "...                                                 ...  \n",
       "9995              lovely yarn fast delivery pleased buy  \n",
       "9996  perfectly match charm ordered make photo penda...  \n",
       "9997                                    perfect project  \n",
       "9998                                      great product  \n",
       "9999  little pricy pen case definitely sturdy fabric...  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_df = train_df.copy()\n",
    "\n",
    "# # Iterate over each column\n",
    "# for column in train_df.columns:\n",
    "#     # Check if the column is not 'overall'\n",
    "#     if column != 'overall':\n",
    "#         # Tokenize values in the column\n",
    "#         tokenized_values = train_df[column].apply(lambda x: word_tokenize(x))\n",
    "#         # Add tokenized values to the new DataFrame\n",
    "#         tokenized_df[column] = tokenized_values\n",
    "\n",
    "# tokenized_df\n",
    "\n",
    "# word2vec_model = gensim.models.Word2Vec(subset_df[\"tokens\"].tolist(), min_count=5, window=9, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use most cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect match charm order make photo pendant like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                  ssprocessing_text\n",
       "0           5  love glitter pen sparkl delight page brilliant...\n",
       "1           5                     work well machin use most cone\n",
       "2           5  great assort color though lot pink mix still c...\n",
       "3           5                                               look\n",
       "4           5                         make 400 bird hospit month\n",
       "...       ...                                                ...\n",
       "9995        5                  love yarn fast deliveri pleas buy\n",
       "9996        5  perfect match charm order make photo pendant like\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  littl prici pen case definit sturdi fabric out...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ss = train_df[['overall', 'ssprocessing_text']].copy()\n",
    "train_df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>psprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkl delight page brilliant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machin use mostli cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>love yarn fast deliveri pleas buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfectli match charm order make photo pendant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>littl prici pen case definit sturdi fabric out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                  psprocessing_text\n",
       "0           5  love glitter pen sparkl delight page brilliant...\n",
       "1           5                   work well machin use mostli cone\n",
       "2           5  great assort color though lot pink mix still c...\n",
       "3           5                                               look\n",
       "4           5                         make 400 bird hospit month\n",
       "...       ...                                                ...\n",
       "9995        5                  love yarn fast deliveri pleas buy\n",
       "9996        5  perfectli match charm order make photo pendant...\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  littl prici pen case definit sturdi fabric out...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ps = train_df[['overall', 'psprocessing_text']].copy()\n",
    "train_df_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>lemprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>love glitter pen sparkle delightfully page bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>work well machine use mostly cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>great assortment color though lot pink mix sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>make 400 bird hospital month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>lovely yarn fast delivery pleased buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>perfectly match charm ordered make photo penda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>perfect project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5</td>\n",
       "      <td>little pricy pen case definitely sturdy fabric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall                                 lemprocessing_text\n",
       "0           5  love glitter pen sparkle delightfully page bri...\n",
       "1           5                  work well machine use mostly cone\n",
       "2           5  great assortment color though lot pink mix sti...\n",
       "3           5                                            looking\n",
       "4           5                       make 400 bird hospital month\n",
       "...       ...                                                ...\n",
       "9995        5              lovely yarn fast delivery pleased buy\n",
       "9996        5  perfectly match charm ordered make photo penda...\n",
       "9997        5                                    perfect project\n",
       "9998        5                                      great product\n",
       "9999        5  little pricy pen case definitely sturdy fabric...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_lem = train_df[['overall', 'lemprocessing_text']].copy()\n",
    "train_df_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "def early_stopping(monitor='val_loss', min_delta=0, patience=5, mode='auto'):\n",
    "    return EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, mode=mode)\n",
    "\n",
    "# Function to create step decay learning rate scheduler\n",
    "def step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5):\n",
    "    def scheduler(epoch):\n",
    "        return initial_lr * np.power(drop_factor, np.floor((1 + epoch) / epochs_drop))\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 35s 257ms/step - loss: 0.8419 - accuracy: 0.7635 - val_loss: 0.7108 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.6401 - accuracy: 0.7790 - val_loss: 0.6854 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 31s 248ms/step - loss: 0.5275 - accuracy: 0.8126 - val_loss: 0.7149 - val_accuracy: 0.7720 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.4330 - accuracy: 0.8465 - val_loss: 0.7334 - val_accuracy: 0.7655 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 31s 248ms/step - loss: 0.3445 - accuracy: 0.8839 - val_loss: 0.8736 - val_accuracy: 0.7565 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 0.2979 - accuracy: 0.9018 - val_loss: 0.9460 - val_accuracy: 0.7320 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 31s 244ms/step - loss: 0.2635 - accuracy: 0.9137 - val_loss: 1.0001 - val_accuracy: 0.7415 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 1.0001 - accuracy: 0.7415\n",
      "Test Accuracy: 0.7415000200271606\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model = LSTM_model(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 30s 220ms/step - loss: 2.1485 - accuracy: 0.7219 - val_loss: 1.9017 - val_accuracy: 0.7015 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 1.9187 - accuracy: 0.7293 - val_loss: 1.7563 - val_accuracy: 0.7680 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 1.9050 - accuracy: 0.7444 - val_loss: 1.7790 - val_accuracy: 0.7715 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 1.8972 - accuracy: 0.7458 - val_loss: 1.6910 - val_accuracy: 0.7695 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 1.7723 - accuracy: 0.7425 - val_loss: 1.6300 - val_accuracy: 0.7440 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 1.6721 - accuracy: 0.7249 - val_loss: 1.5184 - val_accuracy: 0.7610 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 1.5983 - accuracy: 0.7381 - val_loss: 1.4460 - val_accuracy: 0.7630 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 1.5303 - accuracy: 0.7502 - val_loss: 1.4126 - val_accuracy: 0.7735 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 34s 268ms/step - loss: 1.5138 - accuracy: 0.7560 - val_loss: 1.3342 - val_accuracy: 0.7770 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 1.4253 - accuracy: 0.7561 - val_loss: 1.3203 - val_accuracy: 0.7765 - lr: 2.5000e-04\n",
      "63/63 [==============================] - 5s 75ms/step - loss: 1.3203 - accuracy: 0.7765\n",
      "Test Accuracy: 0.7764999866485596\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "def LSTM_model2(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model2 = LSTM_model2(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 37s 268ms/step - loss: 0.8415 - accuracy: 0.7602 - val_loss: 0.7244 - val_accuracy: 0.7815 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.6368 - accuracy: 0.7769 - val_loss: 0.6988 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.5485 - accuracy: 0.8051 - val_loss: 0.7050 - val_accuracy: 0.7735 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 39s 310ms/step - loss: 0.4695 - accuracy: 0.8340 - val_loss: 0.7575 - val_accuracy: 0.7635 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 39s 313ms/step - loss: 0.3863 - accuracy: 0.8686 - val_loss: 0.7594 - val_accuracy: 0.7625 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.3440 - accuracy: 0.8831 - val_loss: 0.8294 - val_accuracy: 0.7605 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 33s 264ms/step - loss: 0.3095 - accuracy: 0.8976 - val_loss: 0.8883 - val_accuracy: 0.7350 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 3s 51ms/step - loss: 0.8883 - accuracy: 0.7350\n",
      "Test Accuracy: 0.7350000143051147\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['psprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model_ps(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model_ps = LSTM_model_ps(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model_ps.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_ps.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 51s 338ms/step - loss: 1.6742 - accuracy: 0.6929 - val_loss: 1.3174 - val_accuracy: 0.7635 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 36s 285ms/step - loss: 1.2595 - accuracy: 0.7534 - val_loss: 1.1400 - val_accuracy: 0.7780 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 1.1417 - accuracy: 0.7628 - val_loss: 1.0605 - val_accuracy: 0.7790 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 33s 262ms/step - loss: 1.0983 - accuracy: 0.7656 - val_loss: 0.9627 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 1.0249 - accuracy: 0.7660 - val_loss: 0.9146 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 1.0078 - accuracy: 0.7663 - val_loss: 0.9288 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 1.0268 - accuracy: 0.7666 - val_loss: 0.9220 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 33s 268ms/step - loss: 0.9992 - accuracy: 0.7663 - val_loss: 0.8940 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.9691 - accuracy: 0.7664 - val_loss: 0.8932 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.9671 - accuracy: 0.7664 - val_loss: 0.8841 - val_accuracy: 0.7800 - lr: 2.5000e-04\n",
      "63/63 [==============================] - 6s 90ms/step - loss: 0.8841 - accuracy: 0.7800\n",
      "Test Accuracy: 0.7799999713897705\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "def LSTM_model2_ps(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model2_ps = LSTM_model2_ps(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2_ps.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2_ps.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 43s 315ms/step - loss: 0.8335 - accuracy: 0.7598 - val_loss: 0.6870 - val_accuracy: 0.7810 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 42s 334ms/step - loss: 0.6322 - accuracy: 0.7778 - val_loss: 0.6746 - val_accuracy: 0.7745 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.5433 - accuracy: 0.8049 - val_loss: 0.6884 - val_accuracy: 0.7595 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 37s 293ms/step - loss: 0.4672 - accuracy: 0.8380 - val_loss: 0.7139 - val_accuracy: 0.7635 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.3869 - accuracy: 0.8635 - val_loss: 0.8104 - val_accuracy: 0.7495 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 0.3475 - accuracy: 0.8802 - val_loss: 0.8480 - val_accuracy: 0.7455 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.3102 - accuracy: 0.8981 - val_loss: 0.8651 - val_accuracy: 0.7615 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 3s 46ms/step - loss: 0.8651 - accuracy: 0.7615\n",
      "Test Accuracy: 0.7615000009536743\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['ssprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def LSTM_model_ss(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model_ss = LSTM_model_ss(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model_ss.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 36s 262ms/step - loss: 1.6636 - accuracy: 0.7376 - val_loss: 1.3586 - val_accuracy: 0.7790 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 1.3196 - accuracy: 0.7589 - val_loss: 1.0429 - val_accuracy: 0.7780 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 1.1402 - accuracy: 0.7581 - val_loss: 0.9925 - val_accuracy: 0.7790 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 1.0932 - accuracy: 0.7641 - val_loss: 0.9945 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 32s 252ms/step - loss: 1.0802 - accuracy: 0.7639 - val_loss: 0.9682 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 1.0356 - accuracy: 0.7635 - val_loss: 0.9279 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 1.0200 - accuracy: 0.7648 - val_loss: 0.9090 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 1.0113 - accuracy: 0.7654 - val_loss: 0.8938 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.9916 - accuracy: 0.7654 - val_loss: 0.8802 - val_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 0.9958 - accuracy: 0.7660 - val_loss: 0.8739 - val_accuracy: 0.7800 - lr: 2.5000e-04\n",
      "63/63 [==============================] - 5s 80ms/step - loss: 0.8739 - accuracy: 0.7800\n",
      "Test Accuracy: 0.7799999713897705\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "def LSTM_model2_ss(input_length, vocab_size, embedding_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model2_ss = LSTM_model2_ss(max_len, vocab_size, embedding_size)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2_ss.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec with Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 37s 275ms/step - loss: 0.8359 - accuracy: 0.7630 - val_loss: 0.6983 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.6254 - accuracy: 0.7797 - val_loss: 0.6832 - val_accuracy: 0.7695 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 33s 264ms/step - loss: 0.5136 - accuracy: 0.8194 - val_loss: 0.7209 - val_accuracy: 0.7755 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 36s 289ms/step - loss: 0.4267 - accuracy: 0.8482 - val_loss: 0.7996 - val_accuracy: 0.7655 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 40s 323ms/step - loss: 0.3263 - accuracy: 0.8870 - val_loss: 0.8729 - val_accuracy: 0.7575 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.2877 - accuracy: 0.9021 - val_loss: 0.9408 - val_accuracy: 0.7515 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.2505 - accuracy: 0.9180 - val_loss: 1.0345 - val_accuracy: 0.7525 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 4s 67ms/step - loss: 1.0345 - accuracy: 0.7525\n",
      "Test Accuracy with Word2Vec embeddings: 0.7524999976158142\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec = LSTM_model_with_Word2Vec(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 54s 403ms/step - loss: 0.8072 - accuracy: 0.7607 - val_loss: 0.6757 - val_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 51s 412ms/step - loss: 0.6132 - accuracy: 0.7807 - val_loss: 0.6791 - val_accuracy: 0.7735 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 50s 397ms/step - loss: 0.5143 - accuracy: 0.8154 - val_loss: 0.7231 - val_accuracy: 0.7570 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 44s 349ms/step - loss: 0.4309 - accuracy: 0.8464 - val_loss: 0.8451 - val_accuracy: 0.7395 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 44s 349ms/step - loss: 0.3334 - accuracy: 0.8836 - val_loss: 0.8849 - val_accuracy: 0.7490 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 44s 349ms/step - loss: 0.2832 - accuracy: 0.9051 - val_loss: 0.9231 - val_accuracy: 0.7535 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 5s 81ms/step - loss: 0.9231 - accuracy: 0.7535\n",
      "Test Accuracy with Word2Vec embeddings: 0.7534999847412109\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model2_with_Word2Vec(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model2_with_word2vec = LSTM_model2_with_Word2Vec(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2_with_word2vec.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2_with_word2vec.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 56s 402ms/step - loss: 0.8222 - accuracy: 0.7631 - val_loss: 0.7021 - val_accuracy: 0.7805 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 45s 364ms/step - loss: 0.6362 - accuracy: 0.7780 - val_loss: 0.6726 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 46s 366ms/step - loss: 0.5507 - accuracy: 0.8020 - val_loss: 0.6895 - val_accuracy: 0.7725 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 46s 366ms/step - loss: 0.4761 - accuracy: 0.8305 - val_loss: 0.7235 - val_accuracy: 0.7645 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 46s 370ms/step - loss: 0.3961 - accuracy: 0.8648 - val_loss: 0.7640 - val_accuracy: 0.7550 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 47s 380ms/step - loss: 0.3537 - accuracy: 0.8800 - val_loss: 0.8138 - val_accuracy: 0.7635 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 46s 371ms/step - loss: 0.3246 - accuracy: 0.8907 - val_loss: 0.8606 - val_accuracy: 0.7520 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 6s 90ms/step - loss: 0.8606 - accuracy: 0.7520\n",
      "Test Accuracy with Word2Vec embeddings: 0.7519999742507935\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and target labels\n",
    "X = train_df['psprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec_ps(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec_ps = LSTM_model_with_Word2Vec_ps(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec_ps.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec_ps.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 91s 353ms/step - loss: 0.7746 - accuracy: 0.7639 - val_loss: 0.6703 - val_accuracy: 0.7810 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.6179 - accuracy: 0.7839 - val_loss: 0.6854 - val_accuracy: 0.7710 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 113s 455ms/step - loss: 0.5237 - accuracy: 0.8123 - val_loss: 0.6996 - val_accuracy: 0.7605 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 120s 480ms/step - loss: 0.4470 - accuracy: 0.8428 - val_loss: 0.7385 - val_accuracy: 0.7660 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 111s 445ms/step - loss: 0.3487 - accuracy: 0.8819 - val_loss: 0.8479 - val_accuracy: 0.7515 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 118s 473ms/step - loss: 0.2969 - accuracy: 0.9045 - val_loss: 0.9272 - val_accuracy: 0.7450 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 8s 119ms/step - loss: 0.9272 - accuracy: 0.7450\n",
      "Test Accuracy with Word2Vec embeddings: 0.7450000047683716\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model2_with_Word2Vec_ps(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model2_with_word2vec_ps = LSTM_model2_with_Word2Vec_ps(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2_with_word2vec_ps.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2_with_word2vec_ps.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Word2Vec with Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 47s 346ms/step - loss: 0.8480 - accuracy: 0.7620 - val_loss: 0.7066 - val_accuracy: 0.7810 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 47s 375ms/step - loss: 0.6386 - accuracy: 0.7768 - val_loss: 0.7015 - val_accuracy: 0.7740 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 49s 393ms/step - loss: 0.5455 - accuracy: 0.8040 - val_loss: 0.6973 - val_accuracy: 0.7680 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 49s 394ms/step - loss: 0.4706 - accuracy: 0.8366 - val_loss: 0.7379 - val_accuracy: 0.7850 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 0.3821 - accuracy: 0.8690 - val_loss: 0.8066 - val_accuracy: 0.7515 - lr: 5.0000e-04\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 45s 357ms/step - loss: 0.3397 - accuracy: 0.8852 - val_loss: 0.8224 - val_accuracy: 0.7640 - lr: 5.0000e-04\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 44s 351ms/step - loss: 0.3060 - accuracy: 0.9003 - val_loss: 0.8903 - val_accuracy: 0.7505 - lr: 5.0000e-04\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 43s 343ms/step - loss: 0.2712 - accuracy: 0.9119 - val_loss: 0.9212 - val_accuracy: 0.7480 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 6s 99ms/step - loss: 0.9212 - accuracy: 0.7480\n",
      "Test Accuracy with Word2Vec embeddings: 0.7480000257492065\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and target labels\n",
    "X = train_df['ssprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec_ss(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100  # Define your embedding size\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec_ss = LSTM_model_with_Word2Vec_ss(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec_ss.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 92s 353ms/step - loss: 0.7863 - accuracy: 0.7659 - val_loss: 0.6774 - val_accuracy: 0.7780 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.6183 - accuracy: 0.7831 - val_loss: 0.6760 - val_accuracy: 0.7785 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 107s 427ms/step - loss: 0.5128 - accuracy: 0.8176 - val_loss: 0.6955 - val_accuracy: 0.7640 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 93s 373ms/step - loss: 0.4344 - accuracy: 0.8494 - val_loss: 0.7311 - val_accuracy: 0.7610 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 89s 356ms/step - loss: 0.3461 - accuracy: 0.8815 - val_loss: 0.7804 - val_accuracy: 0.7550 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 109s 437ms/step - loss: 0.2967 - accuracy: 0.9016 - val_loss: 0.8429 - val_accuracy: 0.7450 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 83s 332ms/step - loss: 0.2612 - accuracy: 0.9172 - val_loss: 0.8901 - val_accuracy: 0.7470 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 6s 100ms/step - loss: 0.8901 - accuracy: 0.7470\n",
      "Test Accuracy with Word2Vec embeddings: 0.746999979019165\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model2_with_Word2Vec_ss(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(200))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model2_with_word2vec_ss = LSTM_model2_with_Word2Vec_ss(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "lstm_model2_with_word2vec_ss.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model2_with_word2vec_ss.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
