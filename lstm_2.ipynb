{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ak2023\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ak2023\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ak2023\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shutil\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text_without_punctuation)\n",
    "    return normalized_text\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text, method='snowballstemming'):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Define the set of stopwords\n",
    "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "                \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
    "                'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "                'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "                'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "                'but', 'if','or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                'against', 'between', 'into', 'through','during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both','each', 'few', 'more', 'most', 'other',\n",
    "                'some', 'such','product', 'purchase', 'buy', 'bought', 'ordered', 'order', 'item', 'items', \n",
    "                'review', 'reviews', 'star', 'stars', 'rating', 'ratings', 'customer', 'customers', \n",
    "                'seller', 'sellers', 'seller', 'brand', 'brands', 'price', 'prices', 'money', \n",
    "                'value', 'quality', 'deal', 'deals','shipping', 'shipping', 'delivery', 'delivered', 'package', \n",
    "                'packaging','customer service', 'service', 'support', 'warranty', 'warranties', \n",
    "                'problem','use','one','work','machine','using','need','needed','cut','recieve',\n",
    "                'glitter','pen','marker','folder','card', 'look','looking','color','background',\n",
    "                'tool','design','made','craft','piece','store','size','markers','page','make','used' }\n",
    "\n",
    "\n",
    "    tokens = [remove_punctuation(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    if method == 'snowballstemming':\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'porterstemming':\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'lemmatization':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['lemprocessing_text'] = train_df['Review'].apply(preprocess_text, method='lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "def early_stopping(monitor='val_loss', min_delta=0, patience=5, mode='auto'):\n",
    "    return EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, mode=mode)\n",
    "\n",
    "# Function to create step decay learning rate scheduler\n",
    "def step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5):\n",
    "    def scheduler(epoch):\n",
    "        return initial_lr * np.power(drop_factor, np.floor((1 + epoch) / epochs_drop))\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "# Define parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
    "embedding_size = 100 \n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model with Word2Vec embeddings\n",
    "def LSTM_model_with_Word2Vec_lem(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "lstm_model_with_word2vec_lem = LSTM_model_with_Word2Vec_lem(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Train the model\n",
    "lstm_model_with_word2vec_lem.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model_with_word2vec_lem.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = lstm_model_with_word2vec_lem.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('lstm_w2v_lem2.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('lstm_w2v_lem2_tm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
