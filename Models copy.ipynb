{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shutil\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dff = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_dff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# import pandas as pd\n",
    "# desired_count = 20000 \n",
    "\n",
    "# print(\"Before undersampling:\", Counter(train_df['overall']))\n",
    "# most_common_class, most_common_count = Counter(train_df['overall']).most_common(1)[0]\n",
    "# sampling_strategy = {label: min(desired_count, count) for label, count in Counter(train_df['overall']).items()}\n",
    "# rus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# undersampled_X, undersampled_y = rus.fit_resample(train_df.drop(columns=['overall']), train_df['overall'])\n",
    "# train_df = pd.concat([pd.DataFrame(undersampled_X), pd.DataFrame({'overall': undersampled_y})], axis=1)\n",
    "# print(\"After undersampling:\", Counter(train_df['overall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text_without_punctuation)\n",
    "    return normalized_text\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text, method='snowballstemming'):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Define the set of stopwords\n",
    "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "                \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
    "                'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "                'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "                'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "                'but', 'if','or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                'against', 'between', 'into', 'through','during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both','each', 'few', 'more', 'most', 'other',\n",
    "                'some', 'such','product', 'purchase', 'buy', 'bought', 'ordered', 'order', 'item', 'items', \n",
    "                'review', 'reviews', 'star', 'stars', 'rating', 'ratings', 'customer', 'customers', \n",
    "                'seller', 'sellers', 'seller', 'brand', 'brands', 'price', 'prices', 'money', \n",
    "                'value', 'quality', 'deal', 'deals','shipping', 'shipping', 'delivery', 'delivered', 'package', \n",
    "                'packaging','customer service', 'service', 'support', 'warranty', 'warranties', \n",
    "                'problem','use','one','work','machine','using','need','needed','cut','recieve',\n",
    "                'glitter','pen','marker','folder','card', 'look','looking','color','background',\n",
    "                'tool','design','made','craft','piece','store','size','markers','page','make','used' }\n",
    "\n",
    "\n",
    "    tokens = [remove_punctuation(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    if method == 'snowballstemming':\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'porterstemming':\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'lemmatization':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_10040\\3045761950.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset='Review',inplace=True)\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_10040\\3045761950.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['ssprocessing_text'] = train_df['Review'].apply(preprocess_text, method='snowballstemming')\n"
     ]
    }
   ],
   "source": [
    "train_df.dropna(subset='Review',inplace=True)\n",
    "train_df['ssprocessing_text'] = train_df['Review'].apply(preprocess_text, method='snowballstemming')\n",
    "# train_df['psprocessing_text'] = train_df['Review'].apply(preprocess_text, method='porterstemming')\n",
    "# train_df['lemprocessing_text'] = train_df['Review'].apply(preprocess_text, method='lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_10040\\1420501431.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset='Review',inplace=True)\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_10040\\1420501431.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset='ssprocessing_text',inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these glitter pens. They sparkle deligh...</td>\n",
       "      <td>love pen sparkl delight brilliant colour even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>It works well with my machine.  I use mostly c...</td>\n",
       "      <td>work well most cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a great assortment of colors, though t...</td>\n",
       "      <td>great assort color though there lot pink mix s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just what I was looking for.</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I make 400 birds for the hospital each month.</td>\n",
       "      <td>400 bird hospit each month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370858</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these dies they make great backgrounds ...</td>\n",
       "      <td>love die great background card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370859</th>\n",
       "      <td>5</td>\n",
       "      <td>I love Darice embossing folders.  Darcie's fol...</td>\n",
       "      <td>love daric emboss folder darci folder veri rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370860</th>\n",
       "      <td>5</td>\n",
       "      <td>I ordered these to add to my Earthy Markers th...</td>\n",
       "      <td>add earthi previous want brighter primari colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370861</th>\n",
       "      <td>4</td>\n",
       "      <td>Made perfect off white color by blending a tin...</td>\n",
       "      <td>perfect white blend tini amount yellow 2 white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370862</th>\n",
       "      <td>5</td>\n",
       "      <td>I used this to make photo canvases for Christm...</td>\n",
       "      <td>photo canvas christma realli good task adher p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367927 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                             Review  \\\n",
       "0             5  I love these glitter pens. They sparkle deligh...   \n",
       "1             5  It works well with my machine.  I use mostly c...   \n",
       "2             5  This is a great assortment of colors, though t...   \n",
       "3             5                       Just what I was looking for.   \n",
       "4             5      I make 400 birds for the hospital each month.   \n",
       "...         ...                                                ...   \n",
       "370858        5  I love these dies they make great backgrounds ...   \n",
       "370859        5  I love Darice embossing folders.  Darcie's fol...   \n",
       "370860        5  I ordered these to add to my Earthy Markers th...   \n",
       "370861        4  Made perfect off white color by blending a tin...   \n",
       "370862        5  I used this to make photo canvases for Christm...   \n",
       "\n",
       "                                        ssprocessing_text  \n",
       "0       love pen sparkl delight brilliant colour even ...  \n",
       "1                                     work well most cone  \n",
       "2       great assort color though there lot pink mix s...  \n",
       "3                                                    just  \n",
       "4                              400 bird hospit each month  \n",
       "...                                                   ...  \n",
       "370858                     love die great background card  \n",
       "370859  love daric emboss folder darci folder veri rea...  \n",
       "370860  add earthi previous want brighter primari colo...  \n",
       "370861  perfect white blend tini amount yellow 2 white...  \n",
       "370862  photo canvas christma realli good task adher p...  \n",
       "\n",
       "[367927 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna(subset='Review',inplace=True)\n",
    "train_df.dropna(subset='ssprocessing_text',inplace=True)\n",
    "# train_df.dropna(subset='psprocessing_text',inplace=True)\n",
    "# train_df.dropna(subset='lemprocessing_text',inplace=True)\n",
    "train_df = train_df[train_df['Review'].str.len() >= 3]\n",
    "train_df = train_df[train_df['ssprocessing_text'].str.len() >= 3]\n",
    "# train_df = train_df[train_df['psprocessing_text'].str.len() >= 3]\n",
    "# train_df = train_df[train_df['lemprocessing_text'].str.len() >= 3]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 50\n",
    "window = 15\n",
    "min_count = 10\n",
    "\n",
    "train_subset = train_df[:]\n",
    "\n",
    "train_data, test_data = train_test_split(train_subset, test_size=0.01, random_state=42)\n",
    "\n",
    "sentences = [text.split() for text in train_data['ssprocessing_text']]\n",
    "model_fasttext = FastText(sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3)) \n",
    "count_vectorizer.fit(train_data['ssprocessing_text'])\n",
    "\n",
    "X_train_tfidf = count_vectorizer.transform(train_data['ssprocessing_text'])\n",
    "X_test_count = count_vectorizer.transform(test_data['ssprocessing_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_embeddings_batch(data, model, batch_size=1000):\n",
    "    embeddings = []\n",
    "    num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_data = data.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "        batch_embeddings = []\n",
    "        for text in batch_data:\n",
    "            tokens = text.split()\n",
    "            token_embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "            if token_embeddings:\n",
    "                batch_embeddings.append(np.mean(token_embeddings, axis=0))\n",
    "            else:\n",
    "                batch_embeddings.append(np.zeros(model.vector_size))\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = generate_embeddings_batch(train_data['ssprocessing_text'], model_fasttext)\n",
    "X_test_embeddings = generate_embeddings_batch(test_data['ssprocessing_text'], model_fasttext)\n",
    "\n",
    "X_train_count_sparse = X_train_tfidf.astype(np.float32)\n",
    "X_test_count_sparse = X_test_count.astype(np.float32)\n",
    "X_train_embeddings_sparse = X_train_embeddings.astype(np.float32)\n",
    "X_test_embeddings_sparse = X_test_embeddings.astype(np.float32)\n",
    "\n",
    "X_train_combined = hstack([X_train_embeddings_sparse, X_train_count_sparse])\n",
    "X_test_combined = hstack([X_test_embeddings_sparse, X_test_count_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=6000,verbose=1,n_jobs=-1)\n",
    "classifier.fit(X_train_combined, train_data['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_combined)\n",
    "report = classification_report(test_data['overall'], y_pred, output_dict=True)\n",
    "\n",
    "results = pd.DataFrame(columns=['Processing', 'N-gram Range', 'Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
    "results.loc[len(results)] = ['Logistic Regression (FastText + TF-IDF)', (1, 3), \n",
    "                             report['accuracy'], report['weighted avg']['precision'], \n",
    "                             report['weighted avg']['recall'], report['weighted avg']['f1-score']]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(test_data['overall'], y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ssprocessing_text'] = test_df['Review'].apply(preprocess_text)\n",
    "test_df['ssprocessing_text'].fillna('',inplace=True)\n",
    "\n",
    "test_embeddings = generate_embeddings_batch(test_df['ssprocessing_text'], model_fasttext)\n",
    "X_test_count = count_vectorizer.transform(test_df['ssprocessing_text'])\n",
    "X_test_combined = hstack([test_embeddings, X_test_count.astype(np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test_combined)\n",
    "\n",
    "test_df['overall'] = y_pred_test\n",
    "test_df.drop(columns=['Review','ssprocessing_text'],inplace=True)\n",
    "test_df.to_csv('snowball_preprocessed_balanced_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
