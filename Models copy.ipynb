{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shutil\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dff = pd.read_csv('train.csv.zip')\n",
    "test_df = pd.read_csv('test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_dff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before undersampling: Counter({5: 283165, 4: 45832, 3: 21470, 1: 10772, 2: 9624})\n",
      "After undersampling: Counter({4: 45832, 5: 45832, 3: 21470, 1: 10772, 2: 9624})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "desired_count = 45832 \n",
    "\n",
    "print(\"Before undersampling:\", Counter(train_df['overall']))\n",
    "most_common_class, most_common_count = Counter(train_df['overall']).most_common(1)[0]\n",
    "sampling_strategy = {label: min(desired_count, count) for label, count in Counter(train_df['overall']).items()}\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "undersampled_X, undersampled_y = rus.fit_resample(train_df.drop(columns=['overall']), train_df['overall'])\n",
    "train_df = pd.concat([pd.DataFrame(undersampled_X), pd.DataFrame({'overall': undersampled_y})], axis=1)\n",
    "print(\"After undersampling:\", Counter(train_df['overall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', text_without_punctuation)\n",
    "    return normalized_text\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text, method='snowballstemming'):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Define the set of stopwords\n",
    "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "                \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
    "                'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "                'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "                'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
    "                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "                'but', 'if','or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                'against', 'between', 'into', 'through','during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both','each', 'few', 'more', 'most', 'other',\n",
    "                'some', 'such','product', 'purchase', 'buy', 'bought', 'ordered', 'order', 'item', 'items', \n",
    "                'review', 'reviews', 'star', 'stars', 'rating', 'ratings', 'customer', 'customers', \n",
    "                'seller', 'sellers', 'seller', 'brand', 'brands', 'price', 'prices', 'money', \n",
    "                'value', 'quality', 'deal', 'deals','shipping', 'shipping', 'delivery', 'delivered', 'package', \n",
    "                'packaging','customer service', 'service', 'support', 'warranty', 'warranties', \n",
    "                'problem','use','one','work','machine','using','need','needed','cut','recieve',\n",
    "                'glitter','pen','marker','folder','card', 'look','looking','color','background',\n",
    "                'tool','design','made','craft','piece','store','size','markers','page','make','used' }\n",
    "\n",
    "\n",
    "    tokens = [remove_punctuation(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    if method == 'snowballstemming':\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'porterstemming':\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    elif method == 'lemmatization':\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset='Review',inplace=True)\n",
    "train_df['ssprocessing_text'] = train_df['Review'].apply(preprocess_text, method='snowballstemming')\n",
    "# train_df['psprocessing_text'] = train_df['Review'].apply(preprocess_text, method='porterstemming')\n",
    "# train_df['lemprocessing_text'] = train_df['Review'].apply(preprocess_text, method='lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_15944\\1420501431.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset='Review',inplace=True)\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_15944\\1420501431.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset='ssprocessing_text',inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>ssprocessing_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these glitter pens. They sparkle deligh...</td>\n",
       "      <td>love pen sparkl delight brilliant colour even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>It works well with my machine.  I use mostly c...</td>\n",
       "      <td>work well most cone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a great assortment of colors, though t...</td>\n",
       "      <td>great assort color though lot pink mix still c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Just what I was looking for.</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I make 400 birds for the hospital each month.</td>\n",
       "      <td>400 bird hospit month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370858</th>\n",
       "      <td>5</td>\n",
       "      <td>I love these dies they make great backgrounds ...</td>\n",
       "      <td>love die great background card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370859</th>\n",
       "      <td>5</td>\n",
       "      <td>I love Darice embossing folders.  Darcie's fol...</td>\n",
       "      <td>love daric emboss folder darci folder veri rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370860</th>\n",
       "      <td>5</td>\n",
       "      <td>I ordered these to add to my Earthy Markers th...</td>\n",
       "      <td>add earthi previous want brighter primari colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370861</th>\n",
       "      <td>4</td>\n",
       "      <td>Made perfect off white color by blending a tin...</td>\n",
       "      <td>perfect white blend tini amount yellow 2 white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370862</th>\n",
       "      <td>5</td>\n",
       "      <td>I used this to make photo canvases for Christm...</td>\n",
       "      <td>photo canvas christma realli good task adher p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367593 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                             Review  \\\n",
       "0             5  I love these glitter pens. They sparkle deligh...   \n",
       "1             5  It works well with my machine.  I use mostly c...   \n",
       "2             5  This is a great assortment of colors, though t...   \n",
       "3             5                       Just what I was looking for.   \n",
       "4             5      I make 400 birds for the hospital each month.   \n",
       "...         ...                                                ...   \n",
       "370858        5  I love these dies they make great backgrounds ...   \n",
       "370859        5  I love Darice embossing folders.  Darcie's fol...   \n",
       "370860        5  I ordered these to add to my Earthy Markers th...   \n",
       "370861        4  Made perfect off white color by blending a tin...   \n",
       "370862        5  I used this to make photo canvases for Christm...   \n",
       "\n",
       "                                        ssprocessing_text  \n",
       "0       love pen sparkl delight brilliant colour even ...  \n",
       "1                                     work well most cone  \n",
       "2       great assort color though lot pink mix still c...  \n",
       "3                                                    just  \n",
       "4                                   400 bird hospit month  \n",
       "...                                                   ...  \n",
       "370858                     love die great background card  \n",
       "370859  love daric emboss folder darci folder veri rea...  \n",
       "370860  add earthi previous want brighter primari colo...  \n",
       "370861  perfect white blend tini amount yellow 2 white...  \n",
       "370862  photo canvas christma realli good task adher p...  \n",
       "\n",
       "[367593 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna(subset='Review',inplace=True)\n",
    "train_df.dropna(subset='ssprocessing_text',inplace=True)\n",
    "# train_df.dropna(subset='psprocessing_text',inplace=True)\n",
    "# train_df.dropna(subset='lemprocessing_text',inplace=True)\n",
    "train_df = train_df[train_df['Review'].str.len() >= 3]\n",
    "train_df = train_df[train_df['ssprocessing_text'].str.len() >= 3]\n",
    "# train_df = train_df[train_df['psprocessing_text'].str.len() >= 3]\n",
    "# train_df = train_df[train_df['lemprocessing_text'].str.len() >= 3]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 50\n",
    "window = 15\n",
    "min_count = 10\n",
    "\n",
    "train_subset = train_df[:]\n",
    "\n",
    "train_data, test_data = train_test_split(train_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "sentences = [text.split() for text in train_data['ssprocessing_text']]\n",
    "model_fasttext = FastText(sentences, vector_size=vector_size, window=window, min_count=min_count)\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3)) \n",
    "count_vectorizer.fit(train_data['ssprocessing_text'])\n",
    "\n",
    "X_train_tfidf = count_vectorizer.transform(train_data['ssprocessing_text'])\n",
    "X_test_count = count_vectorizer.transform(test_data['ssprocessing_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_embeddings_batch(data, model, batch_size=1000):\n",
    "    embeddings = []\n",
    "    num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_data = data.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "        batch_embeddings = []\n",
    "        for text in batch_data:\n",
    "            tokens = text.split()\n",
    "            token_embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "            if token_embeddings:\n",
    "                batch_embeddings.append(np.mean(token_embeddings, axis=0))\n",
    "            else:\n",
    "                batch_embeddings.append(np.zeros(model.vector_size))\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = generate_embeddings_batch(train_data['ssprocessing_text'], model_fasttext)\n",
    "X_test_embeddings = generate_embeddings_batch(test_data['ssprocessing_text'], model_fasttext)\n",
    "\n",
    "X_train_count_sparse = X_train_tfidf.astype(np.float32)\n",
    "X_test_count_sparse = X_test_count.astype(np.float32)\n",
    "X_train_embeddings_sparse = X_train_embeddings.astype(np.float32)\n",
    "X_test_embeddings_sparse = X_test_embeddings.astype(np.float32)\n",
    "\n",
    "X_train_combined = hstack([X_train_embeddings_sparse, X_train_count_sparse])\n",
    "X_test_combined = hstack([X_test_embeddings_sparse, X_test_count_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1100)\n",
    "classifier.fit(X_train_combined, train_data['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processing</th>\n",
       "      <th>N-gram Range</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (FastText + TF-IDF)</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.552193</td>\n",
       "      <td>0.548046</td>\n",
       "      <td>0.552193</td>\n",
       "      <td>0.549588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Processing N-gram Range  Accuracy  Precision  \\\n",
       "0  Logistic Regression (FastText + TF-IDF)       (1, 3)  0.552193   0.548046   \n",
       "\n",
       "     Recall  F1-score  \n",
       "0  0.552193  0.549588  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_combined)\n",
    "report = classification_report(test_data['overall'], y_pred, output_dict=True)\n",
    "\n",
    "results = pd.DataFrame(columns=['Processing', 'N-gram Range', 'Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
    "results.loc[len(results)] = ['Logistic Regression (FastText + TF-IDF)', (1, 3), \n",
    "                             report['accuracy'], report['weighted avg']['precision'], \n",
    "                             report['weighted avg']['recall'], report['weighted avg']['f1-score']]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.67      0.65      1936\n",
      "           2       0.50      0.47      0.49      1869\n",
      "           3       0.48      0.45      0.47      1938\n",
      "           4       0.48      0.48      0.48      1898\n",
      "           5       0.64      0.69      0.66      1910\n",
      "\n",
      "    accuracy                           0.55      9551\n",
      "   macro avg       0.55      0.55      0.55      9551\n",
      "weighted avg       0.55      0.55      0.55      9551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_data['overall'], y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ssprocessing_text'] = test_df['Review'].apply(preprocess_text)\n",
    "test_df['ssprocessing_text'].fillna('',inplace=True)\n",
    "\n",
    "test_embeddings = generate_embeddings_batch(test_df['ssprocessing_text'], model_fasttext)\n",
    "X_test_count = count_vectorizer.transform(test_df['ssprocessing_text'])\n",
    "X_test_combined = hstack([test_embeddings, X_test_count.astype(np.float32)])\n",
    "y_pred_test = classifier.predict(X_test_combined)\n",
    "\n",
    "test_df['overall'] = y_pred_test\n",
    "\n",
    "test_df.to_csv('snowball_preprocessed_balanced_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
