{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Bidirectional, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('preprocessed_train.zip')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "def early_stopping(monitor='val_loss', min_delta=0, patience=5, mode='auto'):\n",
    "    return EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, mode=mode)\n",
    "\n",
    "# Function to create step decay learning rate scheduler\n",
    "def step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5):\n",
    "    def scheduler(epoch):\n",
    "        return initial_lr * np.power(drop_factor, np.floor((1 + epoch) / epochs_drop))\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4622/4622 [==============================] - 3852s 832ms/step - loss: 0.8330 - accuracy: 0.7630 - val_loss: 0.8223 - val_accuracy: 0.7633 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "4622/4622 [==============================] - 3496s 756ms/step - loss: 0.8280 - accuracy: 0.7631 - val_loss: 0.8229 - val_accuracy: 0.7633 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "4622/4622 [==============================] - 21106s 5s/step - loss: 0.8253 - accuracy: 0.7631 - val_loss: 0.8208 - val_accuracy: 0.7633 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "4622/4622 [==============================] - 3077s 666ms/step - loss: 0.8242 - accuracy: 0.7631 - val_loss: 0.8223 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "4622/4622 [==============================] - 3088s 668ms/step - loss: 0.8230 - accuracy: 0.7633 - val_loss: 0.8208 - val_accuracy: 0.7633 - lr: 5.0000e-04\n",
      "2311/2311 [==============================] - 180s 78ms/step - loss: 0.8208 - accuracy: 0.7633\n",
      "Test Accuracy with Word2Vec embeddings: 0.7633305191993713\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split the data into features and target labels\n",
    "X = train_df['lemprocessing_text']\n",
    "y = train_df['overall']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding sequences\n",
    "max_len = 100  # Define your maximum sequence length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "embedding_size = 100  \n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "\n",
    "# Convert target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define BiLSTM model with Word2Vec embeddings\n",
    "def BiLSTM_model_with_Word2Vec_lem(input_length, vocab_size, embedding_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_length, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Bidirectional(LSTM(150)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early stopping and decay callbacks\n",
    "early_stop_callback = early_stopping(patience=5)\n",
    "decay = LearningRateScheduler(step_decay(initial_lr=0.001, drop_factor=0.5, epochs_drop=5))\n",
    "callbacks_list = [early_stop_callback, decay]\n",
    "\n",
    "# Create and compile the LSTM model with Word2Vec embeddings\n",
    "bilstm_model_with_word2vec_lem = BiLSTM_model_with_Word2Vec_lem(max_len, vocab_size, embedding_size, embedding_matrix)\n",
    "\n",
    "# Train the model\n",
    "bilstm_model_with_word2vec_lem.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = bilstm_model_with_word2vec_lem.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy with Word2Vec embeddings:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864/3864 [==============================] - 258s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df['Review'] = test_df['Review'].fillna('')\n",
    "X_test = test_df['Review'] \n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
    "\n",
    "# Predict ratings\n",
    "predicted_labels = bilstm_model_with_word2vec_lem.predict(X_test_padded)\n",
    "predicted_ratings = predicted_labels.argmax(axis=1)\n",
    "\n",
    "# Decode numerical labels back to original categories\n",
    "predicted_sentiments = label_encoder.inverse_transform(predicted_ratings)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('bilstm_lem.csv', index=False)\n",
    "\n",
    "# Combine predictions with IDs\n",
    "tm_df = pd.DataFrame({'id': test_df['id'], 'Review': test_df['Review'], 'overall': predicted_sentiments})\n",
    "\n",
    "# Save predictions\n",
    "tm_df.to_csv('bilstm_tm_lem.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
